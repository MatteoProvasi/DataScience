{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0. Settings](#0) \n",
    "<br></br>\n",
    "## [0.1 Librerie](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Le librerie importate, non presenti nell'altro notebook sono:\n",
    "* `StanfordNERTagger`: libreria sviluppata da Stanford che permette di effettuare un NER sulle recensioni.\n",
    "* `sner`: dato che l'operazione in locale è lenta, con questa libreria è possibile accedere al server di Stanford:\n",
    "    * `Ner`: la funzione si connette al serve per effettuare il NER.\n",
    "    * `POSClient`: stesso meccanismo utilizzato per il POS tagging visto che anche questo metodo è lento in locale.\n",
    "* `StanfordPOSTagger`: per il NER è possibile connettersi al server direttamente con la libreria di Stanford, per il POS tagging è più comodo passare per `nltk`, al cui interno è presente la medesima funzione, ed è più facile accedere al server.\n",
    "* `re`: è una libreria per la modifica di stringhe, permette di fare sostituzioni e si lavorare con alcuni pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from iteration_utilities import deepflatten\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sner import Ner, POSClient\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import stanfordnlp\n",
    "\n",
    "os.chdir('D:/Scraping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [0.2 File Reading](#0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recensioni e titoli\n",
    "imdb_df = pd.read_csv(\"imdb_df.csv\", delimiter=';')\n",
    "movies = pd.read_csv(\"movies titles.csv\", delimiter=';')\n",
    "info = pd.read_csv(\"imdb_info.csv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "# [1. Pre-processing](#1)\n",
    "<br></br>\n",
    "## [1.1 Merge Titoli](#1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Si crea un dizionario coppia-valore con rispettivamente i titoli geolocalizzati e i titoli originali. Si utilizza un dizionario in quanto è il metodo più veloce per assegnare le corrispondenze corrette. <br></br>\n",
    "Dai titoli originali è ancora presente l'elemento caratteristico \"(original title)\" che viene sostituito con una stringa vuota una volta messo come valore nel dizionario. Nel caso in cui si trovassero chiavi duplicate il codice avviserebbe con un print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_titles = {}\n",
    "for i in range(len(movies)):\n",
    "    if movies[\"Translated\"].iloc[i] not in diz_titles.keys():\n",
    "        diz_titles[movies[\"Translated\"].iloc[i]] = str(movies[\"Original\"].iloc[i]).replace(\"(original title)\", \"\")\n",
    "    else:\n",
    "        print(\"Duplicate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Sono state provati diversi metodi per far corrispondere i titoli geolocalizzati con quelli originali. È stata creata una nuova variabile e inizializzata con valori nulli per tutte le righe, successivamente sono stati applicati diversi metodi:\n",
    "* Il metodo di iterazione tramite `.iloc[]` in cui un indice viene fatto scorrere fra tutte le righe del dataset. Questa soluzione è pratica, e sarà utilizzata anche più avanti, quando si devono fare dei confronti o individuare degli elementi all'interno delle righe selezionate, ma non è ottimizzata per la scrittura di valori.\n",
    "* Il metodo `.iterrows()` utilizza la stessa logica ma per inserire i nuovi valori si applica un'ulteriore funzione `.set_value`. La combinazione dei due risulta essere migliore del metodo precedente.\n",
    "* La funzione `.map()` è specifica per l'applicazione iterativa di una funzione, in questo caso trovare le corrispondenze con le chiavi di un dizionario.\n",
    "\n",
    "Con questo ultimo metodo si passa dalle diverse ore stimate con `.iloc` e dalla mezz'ora con `iterrows()` a molto meno di un secondo totale. Prima di effettuare questo assegnamento viene modificata un'opzione di `pandas` per evitare che vengano stampati a video dei warning in quanto di default la libreria stampa un warning quando si sostituiscono valori all'interno di un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 68.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imdb_df[\"Original\"] = \"\"\n",
    "pd.options.mode.chained_assignment = None\n",
    "imdb_df[\"Original\"] = imdb_df[\"Title\"].map(diz_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [1.2 Recensioni Float](#1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Questo blocco di codice è stato inserito in seguito a degli errori che sono apparsi più avanti nelle analisi. Nonostante la funzione di scraping `reviews` avesse delle specifiche sia per quando il testo della recensione fosse stato individuato sia per il caso contrario, alcuni valori risultano essere vuoti. Questi pochi errori sono probabilmente dovuti al fatto che la recensione è stata individuata ma non salvata correttamente. <br></br>\n",
    "Il contenuto di queste celle è stato salvato come un oggetto `float` invece che stringa e l'applicazione della funzione `len()` in seguito porta ad un errore in quanto, a differenza delle stringhe, su Python gli oggetti float non hanno lunghezza. <br></br>\n",
    "\n",
    "Per sistemare queste eccezioni si controlla se la tipologia del commento è float, in caso affermativo la cella viene trasformata in una stringa vuota. In questo caso il metodo `iloc[]` funziona rapidamente in quanto si lavora su poche osservazioni e il valore da sostituire è predefinito. Non c'è differenza di velocità fra questo approccio e quello con `iterrows()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(imdb_df)):\n",
    "    if type(imdb_df[\"Review\"].iloc[i]) == float:\n",
    "         imdb_df[\"Review\"].iloc[i] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [1.3 Pre-Processing Recensioni](#1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La seguente funzione ha il compito di eliminare alcuni tag presenti nei commenti, nello specifico i tag `\\r` e `\\n`. Questi tag sono presenti in coppia `\\r\\n` anche se la loro funzionalità singola è la stessa, ovvero quella di creare una nuova linea. Da una risposta su [stackoverflow](https://stackoverflow.com/questions/14606799/what-does-r-do-in-the-following-script) si apprende che quella che può sembrare una ridondanza è invece richiesto dal protocollo [Telnet](https://tools.ietf.org/html/rfc854)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_back_tags(string):\n",
    "    string = string.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    return (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La funzione `pre_processing` forza l'elemento in ingresso ad essere trattato come una lista e su di essa sono svolte delle operazioni di pre-processing. Le diverse funzioni sono prese dalla libreria `gensim` più nello specifico dal modulo `gensim.parsing`:\n",
    "* `strip_multiple_whitespaces`: nel caso ci siano più spazi bianchi consecutivi questi vengono condensati in uno solo.\n",
    "* `strip_numeric`: rimuove i caratteri numerici da una stringa, funziona sia nel caso di numeri considerati stringhe, sia da termini alfanumerici.\n",
    "* `strip_tags`: rimuove le tag in codice HTML e altri tag come visti in precedenze. \n",
    "* `remove_back_tags`: la funzione definita sopra, è necessaria in quanto non sempre la funzione `strip_tags` identifica correttamente quella coppia di tag.\n",
    "\n",
    "La lista `my_filter` è creata con le funzioni di pre-processing al suo interno; questa è passata come parametro di un'altra funzione, `preprocess_string`, sempre della libreria gensim. Questa funzione è quella di default che effettua il pre-processing nella quale è possibile lasciare le funzioni di base o passare una lista con delle funzioni specifiche come in questo caso. <br></br>\n",
    "Dato che l'oggetto in ingresso viene forzato ad essere una lista si applica lo slicing per accedere a tutti gli elementi uno alla volta, e per ogni termine all'interno di essi viene applicato il pre-processing. Per velocizzare tutte le operazioni viene applicata una list comprehension che restituisce come oggetto proprio una lista. <br></br>\n",
    "Poiché si lavora a livello di singola parola, alla fine delle modifiche si ricrea una stringa corrispondente al testo totale della recensione tramite la funzione `join()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(texts: list):\n",
    "    my_filter = [\n",
    "        gensim.parsing.strip_multiple_whitespaces,\n",
    "        gensim.parsing.strip_numeric,\n",
    "        gensim.parsing.strip_tags,\n",
    "        remove_back_tags\n",
    "    ]\n",
    "\n",
    "    texts = [preprocess_string(x, filters=my_filter) for x in (tqdm(texts[0:len(texts)]))]\n",
    "    texts = [\" \".join((x)) for x in texts if type(x) == list]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La colonna delle recensioni è passata in input alla funzione `pre-processing` e salvata nell'oggetto `text`. Successivamente si elimina dal dataset la vecchia colonna delle recensioni e la si sotituisce con il nuovo output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 784259/784259 [01:53<00:00, 6890.97it/s]\n"
     ]
    }
   ],
   "source": [
    "text = pre_processing(imdb_df[\"Review\"])\n",
    "\n",
    "imdb_df.drop(\"Review\", axis=1, inplace=True)\n",
    "imdb_df[\"Review\"] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [1.4 Gestione Forme Contratte](#1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Un problema del pre-processing sorge quando si deve far fronte alle forme contratte. Tramite le funzioni di rimozione della punteggiatura e caratteri speciali (non ancora utilizzate) le forme contratte avrebbero tutti i caratteri uniti o in altri casi si andrebbero a creare due parole distinte. <br></br> Un ulteriore complicazione sorge dal fatto che la stessa parola può essere rappresentata in due modi differenti, ad esempio `it's` e `it\\'s` sono due forme contratte che compaiono frequentemente nelle recensioni. <br></br>\n",
    "\n",
    "La gestiore di queste parole è fondamentale in quanto uno degli step successivi prevede l'identificazione dei nomi e le parole non riconosciute dal POS-tagger sono automaticamente classificate come nomi. In caso di separazione delle forme contratte, ci sarebbero delle parole non esistenti e dunque identificate come nomi. In aggiunta le forme negative, oltre a non essere riconosciute come parole nel dizionario, perdono anche la loro valenza negativa stravolgendo il sentiment della frase. <br></br>\n",
    "\n",
    "Per sistemare queste forme contratte e scriverle per esteso è stato creato un dizionario dove la chiave è la forma contratta e il valore il numero di volte in cui essa compare. L'algoritmo itera fra tutte le recensioni e con la funzione `split()` crea una lista di parole per ognuna di esse. È necessario trasformare ogni recensione in una lista dato che ogni elemento verrà trattato come una parola intera mentre passando direttamente una stringa l'algoritmo prenderebbe un carattere alla volta. Si controlla se nella parola è presente una contrazione e in caso positivo la si aggiunge al dizionario o si aumenta il conteggio già presente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 784259/784259 [00:27<00:00, 28281.71it/s]\n"
     ]
    }
   ],
   "source": [
    "diz_end = {}\n",
    "for i in tqdm(range(len(imdb_df))):\n",
    "    word_list = imdb_df[\"Review\"].iloc[i].split(\" \")\n",
    "    for word in word_list:\n",
    "        if \"\\'\" in word:\n",
    "            if word in diz_end:\n",
    "                diz_end[word] = diz_end[word] + 1\n",
    "            else:\n",
    "                diz_end[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Con la funzione `Counter` si estraggono le occorrenze più frequenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"it's\", 320913),\n",
       " (\"don't\", 221143),\n",
       " (\"It's\", 198018),\n",
       " (\"didn't\", 147291),\n",
       " (\"doesn't\", 138952),\n",
       " (\"I'm\", 128050),\n",
       " (\"can't\", 93444),\n",
       " (\"isn't\", 87013),\n",
       " (\"I've\", 83683),\n",
       " (\"that's\", 75800)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(diz_end).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "In base ai risultati ottenuti si crea un dizionario in cui le chiavi sono le forme contratte e i valori quelle estese. Sono state considerate solamente i casi con maggiore frequenza e soprattutto quelli in cui fosse presente una negazione. Ci sono tantissime altre occorrenze non considerate, ad esempio il genitivo sassone, ma queste verranno in parte sistemate successivamente durante il pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_diz = {\"it's\":\"it is\", \"don't\":\"do not\", \"It's\":\"it is\", \"didn't\":\"did not\", \"doesn't\":\"does not\", \"I'm\":\"I am\",\n",
    "            \"can't\":\"can not\", \"isn't\":\"is not\", \"I've\":\"I have\", \"that's\":\"that is\", \"wasn't\":\"was not\", \"he's\": \"he is\",\n",
    "            \"you're\":\"you are\", \"there's\":\"there is\", \"couldn't\":\"could not\", \"won't\":\"will not\", \"film's\":\"film is\",\n",
    "            \"I'd\":\"I would\", \"wouldn't\":\"would not\", \"There's\":\"there is\", \"you'll\":\"you will\", \"she's\":\"she is\",\n",
    "            \"Don't\":\"do not\", \"haven't\":\"have not\", \"That's\":\"that is\", \"they're\":\"they are\", \"I'll\":\"I will\", \n",
    "            \"aren't\":\"are not\", \"He's\":\"he is\", \"who's\":\"who is\", \"what's\":\"what is\", \"you've\":\"you have\", \n",
    "            \"movie's\":\"movie is\", \"we're\":\"we are\", \"weren't\":\"were not\", \"character's\":\"character is\", \"hasn't\":\"has not\",\n",
    "            \"shouldn't\":\"should not\", \"She's\":\"she is\", \"you'd\":\"you would\", \"could've\":\"could have\", \"we've\":\"we have\",\n",
    "            \"would've\":\"would have\", \"hadn't\":\"had not\", \"What's\":\"What is\", \"they've\":\"they have\", \"i'm\":\"I am\",\n",
    "            \"You'll\":\"You will\", \"They're\":\"They are\", \"i've\":\"I have\", \"Can't\":\"Can not\", \"he'd\":\"he would\",\n",
    "            \"ain't\":\"is not\", \"Here's\":\"Here is\", \"didn't\":\"did not\", \"We're\":\"We are\", \"You're\":\"You are\", \n",
    "            \"he'll\":\"he will\", \"they'd\":\"they would\", \"isn't.\":\"is not.\", \"should've\":\"should have\", \"here's\":\"here is\",\n",
    "            \"I\\'m'\":\"I am\", \"we'll\":\"we will\", \"they'll\":\"they will\", \"it'll\":\"it will\", \"(I'm\":\"I am\", \n",
    "            \"don't.\":\"do not.\", \"Didn't\":\"Did not\", \"It\\'s'\":\"It is\", \"wasn't.\":\"was not.\", \"doesn't.\":\"does not.\",\n",
    "            \"We've\":\"We have\", \"we'd\":\"we would\", \"You've\":\"You have\", \"(it's\":\"it is\", \"else's\":\"else is\", \n",
    "            \"Doesn't\":\"Does not\", \"Isn't\":\"Is not\", \"i'd\":\"I would\", \"she'd\":\"she would\", \"DON'T\":\"do not\", \n",
    "            \"You'd\":\"You would\", \"she'll\":\"she will\", \"who've\":\"who have\", \"Don\\'t\":\"do not\", \"don't,\":\"do not,\",\n",
    "            \"it'd\":\"it would\", \"Wouldn't\":\"Would not\", \"Couldn't\":\"Could not\", \"i'll\":\"I will\", \"can't.\":\"can not.\",\n",
    "            \"Wasn't\":\"was not\", \"(that's\":\"that is\", \"its'\":\"its\", \"didn't,\":\"did not,\", \"it\\'s\":\"it is\", \n",
    "            \"that'll\":\"that will\", \"(don't'\":\"do not\", \"isn't,\":\"is not,\", \"won't.\":\"will not.\", \"doesn't,\":\"does not\",\n",
    "            \"Haven't\":\"Have not\", \"don\\'t\":\"do not\", \"wasn't,\":\"was not\", \"aren't.\":\"are not\", \"can't,\":\"can not\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "# [2. Frasi](#2.)\n",
    "<br></br>\n",
    "## [2.1 Passaggio da Recensioni a Frasi](#2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Come letto in alcuni paper, una delle tecniche per iniziare una aspect-based sentiment analysis è quella di dividere l'intera recensioni in singole frasi. I caratteri delimitatori presi in considerazione sono `?` e `.`, nel caso in questione sono stati considerati anche i punti esclamativi. <br></br>\n",
    "Nelle seguenti righe viene creata una nuova colonna vuota in cui sono inserite, come liste, le frasi di ogni recensione. In ogni recensione i punti di domanda e i punti esclamativi sono trasfomati in punti che sono poi utilizzati come delimitatore per la funzione `split()`. <br></br>\n",
    "\n",
    "In altri parti del codice si utilizzeranno altre librerie o diverse funzioni per sistemare delle componenti testuali. In questo caso concatenare più funzioni `replace()` risulta essere [il metodo più veloce](https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "784259it [01:02, 12531.50it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_df[\"Sentences\"] = \"\"\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    imdb_df.set_value(index, \"Sentences\", row[\"Review\"].replace(\"?\", \".\").replace(\"!\", \".\").split(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Sempre nei paper è specificato come le frasi duplicate all'interno della stessa recensione debbano essere rimosse. Questo deriva dal fatto che spesso gli utenti per sottolineare ed enfatizzare un concetto possono ripetere la stessa frase più volte influenzando il sentiment globale. Questo passaggio serve anche per eliminare quelle frasi che hanno meno di due caratteri, in sostanza i casi in cui, in presenza di più punti consecutivi, si fossero create delle liste vuote o con una sola lettera. <br></br>\n",
    "\n",
    "Per trovare gli elementi singoli di una lista si utilizza il metodo `set()`, questa funzione può essere applicata direttamente alla lista, ma ha lo svantaggio di non mantenere l'ordine degli elementi, il risultato sarebbero frasi all'interno di una recensione in ordine sparso. Per aggirare questo problema si itera per ogni recensione con `iterrows()` e si inizializza ogni volta un oggetto set vuoto. <br></br>\n",
    "Si crea una list comprehension in cui sono inseriti solamente gli oggetti che non sono presenti nel set inizializzato precedentemente e che hanno lunghezza maggiore di uno. Il risultato è sempre una lista ma con elementi non duplicati. Per velocizzare il processo viene creato un assegnamento locale tramite `seen_add = seen.add` e su questo si effettua il check di presenza o assenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n",
      "784259it [01:11, 11034.93it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_df[\"Sentences2\"] = \"\"\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    imdb_df.set_value(index, \"Sentences2\", [x for x in row[\"Sentences\"] if not (x in seen or seen_add(x)) and len(x) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [2.2 Sostituzione delle Forme Contratte](#2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Prima di procedere con un secondo step di pre-processing, avendo eliminato i duplicati all'interno delle recensioni si passa a sostituire le forme contratte. La logica dietro a questo blocco di codice è quella vista in altre parti, ma è leggermente più articolata in quanto si deve ragionare con più elementi all'interno di una singola recensione:\n",
    "* L'iterazione per ogni recensione è effettuata sempre mediante `iterrows()`, per ogni recensione viene creata una lista, `new_review`.\n",
    "* Si itera per ogni frase all'interno della recensione e si crea una lista in cui ogni elemento è una parola di una frase. Si crea un'uleriore lista, `new_sent`, in cui verrà ricreata la singola frase.\n",
    "* Per ogni parola nella lista della frase si controlla se l'elemento è presente nelle chiavi del dizionario delle abbreviazioni. In caso affermativo nella lista `new_sent` viene inserito il valore corrispondente alla chiave, altrimenti la parola passata in input.\n",
    "* A questo punto `new_sent` contiene le singole parole di una frase, per ritornare ad un'unica frase si applica la funzione `join()` e la stringa risultate viene salvata in `new_review`. Si procede per tutte le frasi della recensione, al termine la lista `new_reviews` conterrà tanti elementi quante erano le frasi originarie della recensione.\n",
    "\n",
    "Finita l'analisi di una recensione, l'elemento `new_rewiew` è salvato in una nuova variabile e si procede per via iterativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  del sys.path[0]\n",
      "784259it [01:58, 6640.01it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_df[\"Sentences3\"] = \"\"\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    new_review = []\n",
    "    for sentence in row[\"Sentences2\"]:\n",
    "        temp_sent = sentence.split(\" \")\n",
    "        new_sent = []\n",
    "        for word in temp_sent:\n",
    "            if word in abbr_diz:\n",
    "                new_sent.append(abbr_diz[word])\n",
    "            else:\n",
    "                new_sent.append(word)\n",
    "        new_review.append(\" \".join(new_sent))\n",
    "    imdb_df.set_value(index, \"Sentences3\", new_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [2.3 Pre-Processing](#2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Per completare il pre-processing è necessario creare un'altra funzione, `reduce_length`, per sistemare alcuni casi di caratteri ripetuti. Non è raro trovare alcuni termini scritti con tante lettere ripetute per sottolinearne la valenza. La seguente funzione rimuove $3$ o più caratteri consecutivi e li riporta a $2$:\n",
    "* Attraverso la libreria `re` e la funzione `compile` si inizializzano i pattern da identificare. L'espressione regolare è composta dai seguenti elementi:\n",
    "    * `(.)`: indica che tutti i caratteri saranno presi in considerazione.\n",
    "    * `\\1`: indica un raggruppamento di ordine $1$, in questo caso è ovvio poiché alla funzione sarà passato un termine alla volta.\n",
    "    * `{2,}`: individua qualsiasi carattere ripetuto $2$ o più volte.\n",
    "* Tramite la funzione `sub` i pattern individuati sono sostituiti, il numero di `\\1` indica il numero di caratteri da mettere come sostituzione, in questo caso $2$.\n",
    "<br></br>\n",
    "\n",
    "Come è intuibile questa correzione non restituisce delle parole necessariamente corrtte, ad esempio nel caso in cui si trovino tre lettere ma la parola corretta ne contiene solo una, da questa funzione si avrà in output una parola errata con una doppia. Alcune delle parole potrebbero essere sistemate immediatamente, ma ridurre il numero di caratteri consecutivi aiuta notevolmente gli algoritmi di spelling check che saranno implementati successivamente in quanto la distanza dal termine originale sarà resa minore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_length(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La seguente funzione, `pre-processing2`, ha lo stesso comportamento della prima: si trasforma l'elemento in input in una stringa e si applicano diverse funzioni di pre-processing. La differenza principale rispetto a prima è che adesso le recensioni sono delle liste con un insieme di frasi, risulta quindi necessario applicare un doppio ciclo for nella list comprehension in modo da scorrere prima per tutte le recensioni e per tutte le frasi nelle recensioni. <br></br>\n",
    "\n",
    "Le funzioni di pre-processing applicate in quest step sono:\n",
    "* `strip_multiple_whitespaces`: già usata in precedenza, rimuove spazi bianchi multipli fra parole. Non dovrebbero essercene ma la si utilizza per sicurezza.\n",
    "* `strip_punctuation`: rimuove i caratteri di punteggiatura. Non è stata usata in precedenza per i motivi già esposti.\n",
    "* `strip_non_alphanum`: rimuove i caratteri speciali, anche all'interno di una stringa.\n",
    "* `reduce_length`: rimuove un carattere ripetuto più di $2$ volte consecutivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing2(texts: list):\n",
    "    my_filter = [\n",
    "        gensim.parsing.strip_multiple_whitespaces,\n",
    "        gensim.parsing.strip_punctuation,\n",
    "        gensim.parsing.strip_non_alphanum,\n",
    "        reduce_length\n",
    "    ]\n",
    "\n",
    "    texts = [preprocess_string(x, filters=my_filter) for unique in tqdm((texts[0:len(texts)])) for x in unique]\n",
    "    texts = [\" \".join((x)) for x in texts]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Si crea una variabile che raccoglie l'output della funzione. Nel passaggio precedente la colonna del dataset era eliminata e sostituita con la nuova variabile creata, in questo caso non è possibile farlo in quanto la lunghezza dei due elementi non è la stessa. L'output della funzione `pre_processing2` è un'unica lista in cui sono presenti tutte le frasi di ogni singola recensione e non una lista di liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 784259/784259 [04:33<00:00, 2871.01it/s]\n"
     ]
    }
   ],
   "source": [
    "text = pre_processing2(imdb_df[\"Sentences3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La soluzione di ritornare ad una lista unica può sembrare non molto pratica, però ha sicuramente dei vantaggi nella semplicità del codice ed è possibile con poche righe ricreare liste di liste con le stringhe pre-processate. <br></br>\n",
    "\n",
    "Sempre attraverso un ciclo con `iterrows()` viene realizzato questo processo: \n",
    "* Fuori dal ciclo si inizializza un contatore.\n",
    "* Per ogni recensione non pre-processata si conta quante siano le frasi al suo interno\n",
    "* Attraverso uno slicing fra il contatore e la lunghezza della recensione si individuano gli elementi corrispondenti nella lista pre-processata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n",
      "784259it [01:04, 12130.38it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_df[\"Sentences4\"] = \"\"\n",
    "cont = 0\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    l = len(row[\"Sentences3\"])\n",
    "    imdb_df.set_value(index, \"Sentences4\", text[cont:(cont+l)])\n",
    "    cont = cont + l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Per evitare di riprodurre tutto il codice ogni volta che viene attivato il kernel, si salva il dataframe con `pickle` e lo si ricaricherà ad ogni nuova sessione. Dal file sono eliminate le variabili con i passaggi intermedi di pre-processing sulle recensioni, lasciando la recensione originale e il risultato finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df_copy = imdb_df\n",
    "del imdb_df_copy[\"Sentences\"]\n",
    "del imdb_df_copy[\"Sentences2\"]\n",
    "del imdb_df_copy[\"Sentences3\"]\n",
    "with open('imdb_df4', 'wb') as fp:\n",
    "    pickle.dump(imdb_df_copy, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdb_df4', \"rb\") as input_file:\n",
    "    imdb_df = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "# [3. Spelling Correction](#3)\n",
    "<br></br>\n",
    "## [3.1 Nomi](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Per effettuare una correzione degli errori di ortografia si devono prima di tutto individuare i termini che sono stati scritti erroneamente. Il compito è facile quando una parola è scritta in maniera errata e si è creato un termine che non esiste in lingua inglese, più complicato quando un errore di battitura crea una parola reale. <br></br>\n",
    "Dovendo lavorare con delle recensioni di film, si troveranno un elevato numero di riferimenti agli attori e alle persone coinvolte nella realizzazione del film. I nomi e cognomi degli attori possono facilmente essere riconosciute come parole errate, anche se non dovrebbero esserlo. Grazie alla lista di attori scaricata in precedenza è possibile filtrare queste parole in modo tale da non analizzarle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Fict_actor</th>\n",
       "      <th>Writer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Irishman (2019)</td>\n",
       "      <td>['Martin Scorsese']</td>\n",
       "      <td>['Robert De Niro', 'Al Pacino', 'Joe Pesci', '...</td>\n",
       "      <td>['Frank Sheeran', 'Jimmy Hoffa', 'Russell Bufa...</td>\n",
       "      <td>['Charles Brandt', 'Steven Zaillian']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cena con delitto - Knives Out (2019)</td>\n",
       "      <td>['Rian Johnson']</td>\n",
       "      <td>['Daniel Craig', 'Chris Evans', 'Ana de Armas'...</td>\n",
       "      <td>['Benoit Blanc', 'Ransom Drysdale', 'Marta Cab...</td>\n",
       "      <td>['Rian Johnson']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>C'era una volta a... Hollywood (2019)</td>\n",
       "      <td>['Quentin Tarantino']</td>\n",
       "      <td>['Leonardo DiCaprio', 'Brad Pitt', 'Margot Rob...</td>\n",
       "      <td>['Rick Dalton', 'Cliff Booth', 'Sharon Tate', ...</td>\n",
       "      <td>['Quentin Tarantino']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Joker (2019)</td>\n",
       "      <td>['Todd Phillips']</td>\n",
       "      <td>['Joaquin Phoenix', 'Robert De Niro', 'Zazie B...</td>\n",
       "      <td>['Arthur Fleck', 'Murray Franklin', 'Sophie Du...</td>\n",
       "      <td>['Todd Phillips', 'Scott Silver', 'Bob Kane', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Ad Astra (2019)</td>\n",
       "      <td>['James Gray']</td>\n",
       "      <td>['Brad Pitt', 'Tommy Lee Jones', 'Ruth Negga',...</td>\n",
       "      <td>['Roy McBride', 'H. Clifford McBride', 'Helen ...</td>\n",
       "      <td>['James Gray', 'Ethan Gross']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  Title               Director  \\\n",
       "0           0                    The Irishman (2019)    ['Martin Scorsese']   \n",
       "1           1   Cena con delitto - Knives Out (2019)       ['Rian Johnson']   \n",
       "2           2  C'era una volta a... Hollywood (2019)  ['Quentin Tarantino']   \n",
       "3           3                           Joker (2019)      ['Todd Phillips']   \n",
       "4           4                        Ad Astra (2019)         ['James Gray']   \n",
       "\n",
       "                                               Actor  \\\n",
       "0  ['Robert De Niro', 'Al Pacino', 'Joe Pesci', '...   \n",
       "1  ['Daniel Craig', 'Chris Evans', 'Ana de Armas'...   \n",
       "2  ['Leonardo DiCaprio', 'Brad Pitt', 'Margot Rob...   \n",
       "3  ['Joaquin Phoenix', 'Robert De Niro', 'Zazie B...   \n",
       "4  ['Brad Pitt', 'Tommy Lee Jones', 'Ruth Negga',...   \n",
       "\n",
       "                                          Fict_actor  \\\n",
       "0  ['Frank Sheeran', 'Jimmy Hoffa', 'Russell Bufa...   \n",
       "1  ['Benoit Blanc', 'Ransom Drysdale', 'Marta Cab...   \n",
       "2  ['Rick Dalton', 'Cliff Booth', 'Sharon Tate', ...   \n",
       "3  ['Arthur Fleck', 'Murray Franklin', 'Sophie Du...   \n",
       "4  ['Roy McBride', 'H. Clifford McBride', 'Helen ...   \n",
       "\n",
       "                                              Writer  \n",
       "0              ['Charles Brandt', 'Steven Zaillian']  \n",
       "1                                   ['Rian Johnson']  \n",
       "2                              ['Quentin Tarantino']  \n",
       "3  ['Todd Phillips', 'Scott Silver', 'Bob Kane', ...  \n",
       "4                      ['James Gray', 'Ethan Gross']  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Quando è stato salvato il dataframe in un `.csv` le liste all'interno delle colonne sono state automaticamente trasformate in delle stringhe poiché in questo formato non è possibile salvare alcune strutture di Python come le liste. Gli elementi, nonostante abbiano la corretta formattazione di una lista, sono in realtà letti come delle stringhe. <br></br>\n",
    "La funzione `flat_list` che segue risolve questo problema e ritorna una lista vera e propria. Gli step della funzione sono:\n",
    "* Si sostituiscono con degli spazi nulli gli elementi caratteristici delle liste che in realtà sono presenti nella stringa, ovvero `[`, `]` e `'`.\n",
    "* Se è presente `,` nella nuova stringa significa che originariamente la lista aveva più elementi, con la funzione `split()` si crea una lista di tanti elementi quanti sono i nomi presenti.\n",
    "* Volta per volta i nomi sono inseriti nella lista. Per la spelling correction serve semplicemente una lista di nomi, per adesso non è necessario mantenere le relazioni fra persone e film.\n",
    "\n",
    "Fra i valori in ingresso della funzione è presente anche un parametro opzionale, `unique`, dove se ha valore $1$ l'elemento restituito è una lista degli elementi unici trovati in tutta la colonna, altrimenti mette in output la lista senza togliere i duplicati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(df_column, unique=1):\n",
    "    new_list = []\n",
    "    for element in df_column:\n",
    "        element = element.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        if \",\" in element:\n",
    "            name_list = element.split(\",\")\n",
    "            for name in name_list:\n",
    "                new_list.append(name)\n",
    "        else:\n",
    "            new_list.append(element)\n",
    "\n",
    "        if unique == 1:\n",
    "            new_list = list(set(new_list))\n",
    "    \n",
    "    return (new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "director = flat_list(info[\"Director\"])\n",
    "actor = flat_list(info[\"Actor\"])\n",
    "fict_actor = flat_list(info[\"Fict_actor\"])\n",
    "writer = flat_list(info[\"Writer\"])\n",
    "\n",
    "info_list = [director, actor, fict_actor, writer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [3.2 Dizionario](#3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Per identificare le parole che esistono realmente, una delle possibili soluzioni è quella di prendere un dizionario esterno e controllare di volta in volta se la parola nella recensione sia nel dizionario o meno. <br></br>\n",
    "Esistono diversi dizionari disponibili, il più completo è forse quello di `StanfordNLP`, una libreria che contiene una pipeline per Natural Language Processing, con diversi step che possono essere implementati anche in questo progetto. \n",
    "<br></br>\n",
    "\n",
    "Ci sono state inizialmente delle difficoltà ad installare questa libreria per via di un problema legato a `PyTorch`, una libreria essenziale per installare alcuni pacchetti. Il sito del pacchetto forniva una finestra interattiva con la quale veniva visualizzato il codice da inserire per installare la libreria a seconda della versione di Python ed altre componenti. Per tutte le combinazioni si presentava sempre un problema nell'ambiente non risolvibile. Per installare correttamente la libreria è stato necessario aggiornare Anaconda alla versione $3.7$.\n",
    "<br></br>\n",
    "\n",
    "Oltre a quello di Stanford, il dizionario più completo reperibile è quello presente su [questo GitHub](https://github.com/dwyl/english-words), un dizionario in lingua inglese con $460'000$ parole. La versione scaricata è quella light in quanto comprende solamente parole senza l'aggiunta di caratteri numerici o speciali che sono già stati rimossi nel pre-processing. Vista la completezza di questo file si è deciso di usare questo come dizionario e `StanfordNLP` per il resto delle analisi.<br></br>\n",
    "\n",
    "Il file salvato nella directory viene letto riga per riga e ogni parola, tolto `\\n` indicatore di nuova riga, viene aggiunta ad un dizionario. La creazione di un dizionario al posto di una lista è per velocizzare il controllo delle parole che avviene al passo successivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 184 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_diz = {}\n",
    "with open(\"words_alpha.txt\") as w:\n",
    "    for line in w:\n",
    "        word_diz[line.strip(\"\\n\")] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Al fine di arricchire il dizionario sono inseriti gli elementi presenti nelle informazioni dei film.\n",
    "* È stata creata in precedenza una lista con dentro le singole liste di attori e delle altre informazioni.\n",
    "* Si passa ogni elemento di ogni lista. Se l'elemento ha uno spazio, nome e cognome, viene spezzato e l'oggetto viene aggiunto al dizionario se non è già presente. Si aggiunge direttamente l'elemento senza spezzarlo se non è presente alcuno spazio.\n",
    "* Gli elementi sono aggiunti sia nella forma minuscola che mantenendo la maiuscola all'inizio. Questa scelta, che aumenta la dimensione del dizionario, è necessaria per i seguenti motivi:\n",
    "    * Non sempre nelle recensioni i nomi sono scritti con la maiuscola. La maggior parte degli utenti è attenta a scrivere correttamente ma tanti altri non lo sono.\n",
    "    * Il modello di `StanfordNLP` non sempre riconosce le forme tutte in minuscolo.\n",
    "<br></br>\n",
    "\n",
    "In questo caso il valore assegnato alla chiave è sempre unitario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in info_list:\n",
    "    for element in info:\n",
    "        if \" \" in element:\n",
    "            temp = element.split(\" \")\n",
    "            for token in temp:\n",
    "                if token.lower() not in word_diz.keys():\n",
    "                    word_diz[token.lower()] = 1\n",
    "                    word_diz[token] = 1\n",
    "        else:\n",
    "            if element.lower() not in word_diz.keys():\n",
    "                word_diz[element.lower()] = 1\n",
    "                word_diz[element] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Sicuramente la libreria `StanfordNLP` è più completa, ma dal profilo prestazionale richiede più tempo rispetto ad altre. L'idea è quindi di fare una prima analisi con il dizionario appena letto e controllare successivamente l'insieme ridotto di parole che non sono state riconosciute. \n",
    "\n",
    "Le parole che non esistono nel dizionario sono dei possibili candidati ad essere dei termini scritti erroneamente o dei NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 784259/784259 [01:05<00:00, 12024.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unknown_words = {}\n",
    "for i in tqdm(range(len(imdb_df))):\n",
    "    for sentence in imdb_df[\"Sentences4\"].iloc[i]:\n",
    "        temp = sentence.split(\" \")\n",
    "        for word in temp:\n",
    "            if word.lower() not in word_diz.keys():\n",
    "                if word in unknown_words.keys():\n",
    "                    unknown_words[word] += 1\n",
    "                else:\n",
    "                    unknown_words[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [3.3 NER](#3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Sono caricati i modelli in lingua inglese dalla libreria `StanfordNLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "Y\n",
      "\n",
      "Default download directory: C:\\Users\\Hp\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\Hp\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 235M/235M [04:04<00:00, 962kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\Hp\\stanfordnlp_resources\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    " \n",
    "In un primo momento la fase di NER è stata effettuata importando la libreria `StanfordNERTagger` dal paccketto `nltk`. Questa via prevedeva la creazione di un tagger che prendeva in input i seguenti elementi:\n",
    "* Un modello già addestrato\n",
    "* Il file tagger presente nello .zip scaricato da StanfordNLP\n",
    "* La codifica\n",
    "\n",
    "Il file `english.all.3class.distsim.crf.ser.gz` non è altro che un file compresso al cui interno è presente come unico elemento il modello già addestrato. Il secondo valore in ingresso è `stanford-ner.jar`, il file Java che permette di connettersi al modello e di identificare le eventuali entità all'interno del testo che verrà inserito. Per la codifica è stato scelto `utf-8`, che è già quello di default, e permette di leggere correttamente anche eventuali caratteri stranieri. <br></br>\n",
    "\n",
    "Come ultimo passaggio è stato necessario specificare la directory in cui è installato Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz', 'stanford-ner.jar', encoding='utf-8') \n",
    "\n",
    "java_path = \"C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath\\\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "In maniera molto semplice si scorre fra tutte le chiavi del dizionario e si applica la funzione definita sopra. Per sicurezza la parola viene trasformata in una stringa e applicato il metodo `split()` per renderla una lista, altrimenti la funzione interpreterebbe ogni singolo carattere dell'input come una parola. <br></br>\n",
    "L'output viene aggiunto alla lista. Il formato dell'output è una lista al cui interno sono contenute una serie di tuple, tante quante le parole date in ingresso, in questo caso passando una parola alla volta c'è una tupla sola:\n",
    "* Il primo elemento è la parola in ingresso.\n",
    "* Il secondo elemento della tupla è l'entità identificata dal modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = []\n",
    "for word in tqdm(unknown_words.keys()):\n",
    "    tag = st.tag(str(word).split())\n",
    "    ner.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "L'algoritmo funziona correttamente, ma soffre di estrema lentezza computazionale: su questa macchina la velocità media era di $2$&ndash;$3$ parole al secondo, con un tempo stimato totale di oltre $100$ ore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### [3.3.1 tqdm Output](#3.3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La libreria `tqdm` è spesso utilizzata in questo notebook perché permette di calcolare il tempo di esecuzione di una funzione che itera su un oggetto di lunghezza definita. L'output dovrebbe essere una barra che si completa al passare delle iterazioni. <br></br>\n",
    "Se un blocco di codice però viene interrotto e quindi la barra non raggiunge la fine, quando viene richiamata la funzione, anche in un altro blocco di codice, è possibile che invece di visualizzare una sola riga, la barra di completamento sia stampata a ripetizione. Oltre ad essere un problema grafico, il print continuo di nuovi elementi rallenta leggermente il codice. Il primo metodo che richiedeva $100$ è stato bloccato e questo ha creato il problema nella funzione.\n",
    "<br></br>\n",
    "Importare nuovamente la libreria non ha effetto, ma, come evidenziato in [questa discussione su Github](https://github.com/tqdm/tqdm/issues/375) è possibile sistemare il problema:\n",
    "* La prima soluzione è usare la funzione già presente nel pacchetto, `tqdm._instances.clear()`, con il quale viene ripulita la \"memoria\" della funzione delle precedenti operazioni eseguite. Funziona parzialmente in quanto alcune volte vengono stampate ancora delle nuove linee.\n",
    "* Il secondo metodo è ridefinire la funzione eliminando la memoria di quello passato attraverso un nuovo metodo non presente nel pacchetto originale. Anche in questo caso la memoria non tutte le volte viene ripulita correttamente.\n",
    "* Il terzo metodo consiste nel chiamare la funzione del punto precedente ed importare nuovamente la libreria. Dato che la funzione precedente ha di fatto sovrascritto il nome, questa volta importare la libreria è come se lo si facesse per la prima volta e quindi risolve il problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tqdm(*args, **kwargs):\n",
    "    from tqdm import tqdm as tqdm_base\n",
    "    if hasattr(tqdm_base, '_instances'):\n",
    "        for instance in list(tqdm_base._instances):\n",
    "            tqdm_base._decr_instances(instance)\n",
    "    return tqdm_base(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### [3.3.2 NLP Server](#3.3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "  \n",
    "Fortunatamente è possibile velocizzare notevolmente i tempi computazionali, come riportato in [questa discussione su Stackoverflow](https://stackoverflow.com/questions/57424885/how-to-speedup-stanford-nlp-in-python) è possibile stabilire una connessione con il `Stanford NLP Server` che risulta essere la soluzione ottimale per svolgere un NER. <br></br>\n",
    "L'operazione richiede due passaggi:\n",
    "* Il primo è da effettuarsi sul prompt dei comandi. Selezionata la directory in cui è stato estratto lo .zip scaricato dal sito di Stanford si immette il seguente codice `java -Djava.ext.dirs=./lib -cp stanford-ner.jar edu.stanford.nlp.ie.NERServer -port 9199 -loadClassifier ./classifiers/english.all.3class.distsim.crf.ser.gz`. In questo modo si accede al file `stanford-ner.jar` che è lo stesso specificato nel metodo precedente, nella parte successiva si chiama la connessione al NERServer attraverso una porta ed infine si carica lo .zip del modello, anche in questo caso è lo stesso specificato nel primo metodo.  \n",
    "* Se i file sono stati trovati, il prompt li caricherà e si potrà chiamare la libreria `Ner` che contiene l'omonima classe che permette di fare entity recognition collegandosi ad un server esterno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Ner(host='localhost', port=9199)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Il codice è simile al metodo precedente, in questo caso viene utilizzata la funzione `get_entities` per richiamare l'output del NER. Anche in questo caso la struttura dell'output è la stessa: una lista con dentro tante tuple quante sono le parole in ingresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 219420/219420 [02:32<00:00, 1436.86it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_words = []\n",
    "for word in tqdm(unknown_words.keys()):\n",
    "    ner_words.append(tagger.get_entities(word.split()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La connessione con il server riduce drasticamente i tempi di computazione che ritornano ad essere accettabili. Classificate le entità, si scorre all'interno di ogni elemento, si divide la tupla e si prende l'entità identificata. Si effettua un conteggio per valutare le frequenze delle entità. <br></br>\n",
    "L'entità `O` indica una parola che non è stata riconosciuta come nessuna entità, e sono anche le più numerose. L'altra metà sono risconosciute come delle persone mentre hanno una frequenza molto inferiore i luoghi e le organizzazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\" 'O')\", 114246),\n",
       " (\" 'PERSON')\", 101644),\n",
       " (\" 'LOCATION')\", 2372),\n",
       " (\" 'ORGANIZATION')\", 1158)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = []\n",
    "for entity in ner_words:\n",
    "    temp = str(entity[0]).split(\",\")\n",
    "    idx.append(temp[1])\n",
    "Counter(idx).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Analizzando anche solo i primi $100$ risultati si notano diversi errori di identificazione:\n",
    "* Entità come `Porsche` o `Facebook` non sono riconosciute.\n",
    "* `Nascar` è classificata come persona anche se dovrebbe rientrare sotto organizzazione.\n",
    "* La parola `Motorport` è proprio un errore di spelling ma è classificata come persona.\n",
    "* Alcune sigle come `DVD` non sono riconosciute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CGI', 'O') ('Imax', 'O') ('tifoso', 'O') ('sportcars', 'O') ('Porsche', 'O') ('Corvair', 'PERSON') ('Chevelle', 'PERSON') ('racecar', 'O') ('Nascar', 'PERSON') ('Surtees', 'PERSON') ('autoracing', 'O') ('Biopics', 'O') ('goosebumps', 'O') ('IMAX', 'O') ('Phedon', 'O') ('Cineworld', 'PERSON') ('UK', 'LOCATION') ('minivans', 'O') ('SUVs', 'O') ('Papamichael', 'PERSON') ('McCusker', 'PERSON') ('Beltrami', 'PERSON') ('fearsomly', 'O') ('Facebook', 'ORGANIZATION') ('Audouy', 'PERSON') ('Orlandi', 'PERSON') ('RPMs', 'O') ('WW', 'O') ('racerturned', 'O') ('Brummie', 'PERSON') ('pulsequickening', 'O') ('screentime', 'O') ('motorsport', 'O') ('Petrolheads', 'O') ('skeeted', 'O') ('BGM', 'O') ('Eventhough', 'O') ('biopics', 'O') ('Iacocco', 'PERSON') ('bullxx', 'O') ('groundbreaking', 'O') ('alot', 'O') ('Caitrona', 'PERSON') ('autobody', 'O') ('Corroll', 'PERSON') ('Shellby', 'PERSON') ('Porsches', 'O') ('Ferraris', 'O') ('Broadley', 'PERSON') ('WTF', 'O') ('brummie', 'O') ('Magild', 'PERSON') ('ther', 'O') ('Berthnal', 'PERSON') ('protoptype', 'O') ('mesmerising', 'O') ('VFX', 'O') ('Yaawn', 'PERSON') ('mega', 'O') ('othered', 'O') ('esque', 'O') ('wifey', 'O') ('motorsports', 'O') ('excitig', 'O') ('exasperingly', 'O') ('explaiins', 'O') ('Motorsports', 'O') ('Motorport', 'PERSON') ('underwhelming', 'O') ('reall', 'O') ('petrolhead', 'O') ('fairytale', 'O') ('Buttersworth', 'PERSON') ('1/2', 'O') ('smartphones', 'O') ('Remmington', 'PERSON') ('highspeed', 'O') ('ASR', 'O') ('unjumble', 'O') ('etra', 'O') ('Womens', 'O') ('constructet', 'O') ('everytime', 'O') ('DVD', 'O') ('Daytona', 'PERSON') ('Copland', 'PERSON') ('hotspot', 'O') ('stuntmens', 'O') ('emmersed', 'O') ('Sarthe', 'PERSON') ('Mulsanne', 'O') ('kmh', 'O') ('Chelby', 'PERSON') ('kinda', 'O') ('filmmakers', 'O') ('Ickx', 'PERSON') ('Carrol', 'PERSON') ('FvF', 'O') ('swjs', 'O') ('Sundance', 'O') \n"
     ]
    }
   ],
   "source": [
    "temp_str = ''\n",
    "for ner in ner_words[0:100]:\n",
    "    temp_str += str(ner)[1:-1] + ' '\n",
    "print(temp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### [3.3.4 NER su Frasi](#3.3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Una possibile spiegazione può essere data dal fatto che i modelli sono allenati su delle frasi e non delle singole parole, è quindi possibile che in alcuni casi il modello non riconosce la parola ma ha un'alta confidenza che si tratti di una persona, o qualche altra entità, e la classifica di conseguenza. <br></br>\n",
    "\n",
    "Partendo dalla lista di parole sconosciute si controlla frase per frase all'interno delle recensioni se almeno una di esse è presente. In caso affermativo tutta la frase viene passata alla funzione di NER e l'output viene salvato in una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784259it [34:35, 377.85it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_ner = []\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    for sentence in row[\"Sentences4\"]:\n",
    "        if any(word in unknown_words.keys() for word in sentence.split()):\n",
    "            sentence_ner.append(tagger.get_entities(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Ottenere le entità dalle frasi richiede molto più tempo anche utilizzando la connessione del server, questa è anche una delle ragioni per cui è impraticabile usare questo metodo sull'interno dataset. <br></br>\n",
    "Il risultato viene salvato e caricato per evitare di ripetere il procedimento ogni volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('sentence_ner', 'wb') as fp:\n",
    "    pickle.dump(sentence_ner, fp)\n",
    "\"\"\"\n",
    "    \n",
    "with open('sentence_ner', \"rb\") as input_file:\n",
    "    sentence_ner = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Un modo veloce per controllare se la categorizzazione è la stessa è controllare la lunghezza della lista con gli elementi unici. Se una parola viene classificata come più entità diverse a seconda del contesto, la parola comparirà più volte e di conseguenza la lista sarà più lunga di quella precedente, che era di $219'420$ elementi come era visibile dall'output di `tqdm`. <br></br>\n",
    "\n",
    "La funzione `reduce_length` ha permesso di scendere da 223 mila parole e di identificare un numero maggiore di luoghi e organizzazioni, rendendo la procedura più veloce ed accurata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1066180/1066180 [00:05<00:00, 185028.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "244215"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_words_sent = []\n",
    "for element in tqdm(sentence_ner):\n",
    "    for word in element:\n",
    "        if word[0] in unknown_words.keys():\n",
    "            ner_words_sent.append(word)\n",
    "            \n",
    "len(set(ner_words_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La categorizzazione attraverso le frasi restituisce risultati differenti rispetto a quella per singola parola. <br></br>\n",
    "Dalle entità trovate sono create due liste, una in cui sono messi i termini e nella seconda l'entità relativa. Gli oggetti sono inseriti ad ogni iterazione per mantenere la corrispondenza degli indici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1066180/1066180 [00:04<00:00, 213749.84it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_words_sent = []\n",
    "ent_words_sent = []\n",
    "for element in tqdm(sentence_ner):\n",
    "    for word in element:\n",
    "        if word[0] in unknown_words.keys():\n",
    "            ner_words_sent.append(word[0])\n",
    "            ent_words_sent.append(word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Dalle due liste si crea un dataframe con parola ed entità relativa. Mediante `groupby` e `size()` si crea un nuovo dataframe con il conteggio di quante volte una parola è identificata come una certa identità. Si ordina infine il dataframe in modo descrescente per la nuova colonna di conteggio tramite la funzione `sort_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Entity</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32128</td>\n",
       "      <td>DVD</td>\n",
       "      <td>O</td>\n",
       "      <td>28818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63496</td>\n",
       "      <td>IMDb</td>\n",
       "      <td>O</td>\n",
       "      <td>17886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21167</td>\n",
       "      <td>CGI</td>\n",
       "      <td>O</td>\n",
       "      <td>17013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183503</td>\n",
       "      <td>filmmakers</td>\n",
       "      <td>O</td>\n",
       "      <td>13173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168374</td>\n",
       "      <td>cliché</td>\n",
       "      <td>O</td>\n",
       "      <td>12919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168382</td>\n",
       "      <td>clichés</td>\n",
       "      <td>O</td>\n",
       "      <td>9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197632</td>\n",
       "      <td>kinda</td>\n",
       "      <td>O</td>\n",
       "      <td>8435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193601</td>\n",
       "      <td>indie</td>\n",
       "      <td>O</td>\n",
       "      <td>6611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17296</td>\n",
       "      <td>Bollywood</td>\n",
       "      <td>O</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168375</td>\n",
       "      <td>clichéd</td>\n",
       "      <td>O</td>\n",
       "      <td>6029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191547</td>\n",
       "      <td>http</td>\n",
       "      <td>O</td>\n",
       "      <td>5012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238586</td>\n",
       "      <td>ve</td>\n",
       "      <td>O</td>\n",
       "      <td>5003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147036</td>\n",
       "      <td>WWII</td>\n",
       "      <td>O</td>\n",
       "      <td>4660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141889</td>\n",
       "      <td>UK</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>4349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98033</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>4331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107960</td>\n",
       "      <td>Pixar</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>3367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162965</td>\n",
       "      <td>blogspot</td>\n",
       "      <td>O</td>\n",
       "      <td>3245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147017</td>\n",
       "      <td>WW</td>\n",
       "      <td>O</td>\n",
       "      <td>2834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18578</td>\n",
       "      <td>Braveheart</td>\n",
       "      <td>O</td>\n",
       "      <td>2655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160383</td>\n",
       "      <td>backstory</td>\n",
       "      <td>O</td>\n",
       "      <td>2488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word        Entity  count\n",
       "32128          DVD             O  28818\n",
       "63496         IMDb             O  17886\n",
       "21167          CGI             O  17013\n",
       "183503  filmmakers             O  13173\n",
       "168374      cliché             O  12919\n",
       "168382     clichés             O   9762\n",
       "197632       kinda             O   8435\n",
       "193601       indie             O   6611\n",
       "17296    Bollywood             O   6600\n",
       "168375     clichéd             O   6029\n",
       "191547        http             O   5012\n",
       "238586          ve             O   5003\n",
       "147036        WWII             O   4660\n",
       "141889          UK      LOCATION   4349\n",
       "98033      Netflix  ORGANIZATION   4331\n",
       "107960       Pixar  ORGANIZATION   3367\n",
       "162965    blogspot             O   3245\n",
       "147017          WW             O   2834\n",
       "18578   Braveheart             O   2655\n",
       "160383   backstory             O   2488"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_df = pd.DataFrame({\"Word\":ner_words_sent, \"Entity\":ent_words_sent})\n",
    "\n",
    "unk_df = unk_df.groupby([\"Word\", \"Entity\"]).size().reset_index(name=\"count\").sort_values(by=[\"count\"], ascending=False)\n",
    "unk_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Un dizionario è creato e prenderà come chiave la parola non riconosciuta e come valore l'entità più frequente ad essa associata. Questa soluzione è un'approssimazione in quanto non si ha la certezza che ad una parola sia attribuita l'entità corretta, però si dovrebbe avere una discreta accuratezza.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244215it [00:19, 12590.04it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_diz = {}\n",
    "for index, row in tqdm(unk_df.iterrows()):\n",
    "    if row[\"Word\"] not in entity_diz.keys():\n",
    "        entity_diz[row[\"Word\"]] = row[\"Entity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Se il più delle volte una parola viene riconosciuta come una persona, un'organizzazione o una località, difficilmente si tratta di un errore di battitura, ma di un'entità realmente esistente ma non presente nel dizionario caricato all'inizio. Seguendo questa logica si crea un dizionario uguale al precedente con l'unica differenza che vengono salvati solo quei termini non identificabili con nessuna entità specifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_diz_small = {}\n",
    "for key in entity_diz.keys():\n",
    "    if entity_diz[key] == \"O\":\n",
    "        entity_diz_small[key] = entity_diz[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [3.4 Librerie di Spelling](#3.4) \n",
    "<br></br>\n",
    "<font size=3>\n",
    "    \n",
    "Sono state provate diverse librerie per la spelling correction. Di seguito sono riportati i risultati in termini computazionali delle analisi sulle prime $10$ recensioni. Le valutazioni saranno effettuate sia sulla base della velocità, sia sulla struttura dell'algoritmo implementato in quanto una delle note problematiche di questi algoritmi è la lunghezza computazionale. <br></br>\n",
    "\n",
    "Per ottenere delle tempistiche più accurate, il caricamento delle librerie e le necessarie impostazioni da definire per utilizzare correttamente le funzioni sono effettuate in righe di codice separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "La prima libreria è `Word` presa dal pacchetto `textblob`. Il metodo prende in input una parola alla volta e prova a correggerla mettendo in output una lista di tuple con i possibili candidati e la loro probabilità. <br></br>\n",
    "In [questa pagina](http://norvig.com/spell-correct.html) è presente la documentazione della strategia utilizzata. In sintesi la libreria ha un modello e un dizionario con delle parole e le rispettive frequenze, per ogni parola viene calcolata la probabilità che essa sia una parola nel dizionario. Questa probabilità che viene restituita in output è ponderata per la frequenza della parola nel dizionario e l'edit distance relativa. <br></br>\n",
    "\n",
    "Secondo [questi](https://www.clips.uantwerpen.be/pages/pattern-en#spelling) risultati, citati anche nella pagina del pacchetto, la libreria dovrebbe avere un'accuratezza del $70\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    for sentence in imdb_df[\"Sentences4\"].iloc[i]:\n",
    "        temp = sentence.split()\n",
    "        for word in temp:\n",
    "            check = Word(word.lower())\n",
    "            spelling = check.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "La [seconda libreria](https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html) è `suggest` che lavora nello stesso identico modo di quella precedente. Anche se proviene da un paccheto diverso, la [documentazione](https://www.clips.uantwerpen.be/pages/pattern-en) rimanda alla stessa pagina di quello precedente. Potrebbe quindi trattarsi della stessa identica funzione o una simile con qualche modifica. <br></br>\n",
    "\n",
    "Nel link ad un esempio di applicazione della libreria viene utilizzato il modulo `spelling` che però risulta assente. Dai commenti al post si suggerisce di utilizzare `suggest` che è anche la libreria usata nella documentazione di `pattern.en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    for sentence in imdb_df[\"Sentences4\"].iloc[i]:\n",
    "        temp = sentence.split()\n",
    "        for word in temp:\n",
    "            correct_word = suggest(word.lower()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "La terza libreria `SpellChecker` fa anch'essa riferimento al metodo visto precedentemente, ma a differenza degli altri prende in input una frase intera e non una singola parola. Non dovrebbero esserci differenze fondamentali nel modo in cui l'algoritmo lavora, ma si guadagna molto in termini di tempo. <br></br>\n",
    "\n",
    "Un'altra differenza dalle due librerie prencedenti è l'obbligo di inizializzare la classe per poter applicare l'algorimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    for sentence in imdb_df[\"Sentences4\"].iloc[i]:\n",
    "        for word in sentence.split(\" \"):\n",
    "            spell.correction(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "L'ultimo pacchetto è `SymSpell` introdotto recentemente, prende in prestito l'approccio precedente lo modifica rendendolo fino a [1000 volte](https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f) più veloce e più scalabile all'aumentare della distanza da considerare. <br></br>\n",
    "\n",
    "L'<b>edit distance</b> rappresenta il numero di modifiche da effettuare per ricondursi ad un termine esistente. Tutti i metodi precedenti effettuavano le seguenti operazioni:\n",
    "* Rimozione\n",
    "* Trasposizione (di due caratteri adiacenti)\n",
    "* Sostituzione\n",
    "* Inserimento\n",
    "\n",
    "Il valore dell'edit distance è calcolato in base alla formula di [Damerau–Levenshtein](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance), ogni operazione comporta l'aumento unitario dell'edit distance; se una stessa operazione viene ripetuta due volte, l'edit distance aumenta di altrettanti valori. Il valore dell'edit distance è fissato a $2$ nel caso delle prime due librerie, su `SpellChecker` è sempre due ma può essere ridotto a $1$. `SymSpell` permette invece di considerare qualsiasi valore come parametro in ingresso. <br></br>\n",
    "\n",
    "1. Gli approcci precedenti creano un'elevata quantità di possibili termini da confrontare che esplode all'aumentare dell'edit distance massima.\n",
    "2. Questo metodo utilizza solo la rimozione dei caratteri diminuendo notevolmente i tempi di lavoro. La rimozione viene effettuata in una fase di <b>pre-compilazione</b> sul dizionario e successivamente sui termini da correggere, in questa maniera si riduce drasticamente il numero di possibili candidati all'interno dell'edit distance massima.\n",
    "\n",
    "Nella [repository GitHub](https://github.com/wolfgarbe/SymSpell) è presente in dettaglio la spiegazione del metodo e i confronti computazionali con altre tecniche. <br></br>\n",
    "\n",
    "Per poter usufruire del metodo è necessario scaricare la libreria `pkg_resources`. Nelle altre righe viene inizializzato il dizionario, con le specifiche di pre-compilazione, che verrà successivamente passato alla funzione di correzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    for sentence in imdb_df[\"Sentences4\"].iloc[i]:\n",
    "        suggestions = sym_spell.lookup_compound(sentence, max_edit_distance=2, transfer_casing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "I test effettuati confermano quello che gli articoli sostenevano, ovvero che l'ultimo metodo è estremamente più veloce di tutti gli altri. I primi due metodi sono pressoché identici, quindi è logico supporre che la funzione di base sia la stessa, con qualche modifica minore. Il terzo metodo nonostante sia basato sullo stesso funzionamento impiega la metà del tempo. Infine l'ultimo metodo è $5$ volte più veloce del terzo e $10$ rispetto ai primi due."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "L'ultimo algoritmo sembra l'unico a poter compiere il suo lavoro in tempi ragionevoli, sarà quindi l'unico metodo applicato in questa fase. Il codice per effettuare la spelling correction è il seguente:\n",
    "* Viene inzializzata una lista in cui ogni elemento sarà una lista con dentro ogni frase di una recensione. Come già effettuato questa lista diventerà una nuova colonna del dataset.\n",
    "* Si itera per ogni riga e si crea una lista temporanea. Queste liste conterranno le frasi corrette di una recensione e saranno ripulite ad ogni iterazione.\n",
    "* Come fatto in precedenza si cerca se almeno una parola da correggere è all'interno della frase mediante la funzione `any`. In questo caso si utilizza `entity_diz_small` che contiene solamente le parole non riconosciute dall'algoritmo di NER.\n",
    "    * Se si trova almeno una parola, tutta la frase viene passata al correttore e passando per ogni elemento dell'output si ha la stringa corretta che viene aggiunta alla lista temporanea.\n",
    "    * Se nessuna parola è presente allora si aggiunge la frase originale.\n",
    "* Alla fine di una iterazione la lista temporanea è aggiunta alla lista principale e si procede per via iterativa.\n",
    "\n",
    "Per la correzione si utilizza una edit distance massima di $2$ e con il parametro `transfer_casing=True` si forza l'algoritmo ha mettere in output una parola che mantenga le stesse maiuscole/minuscole del termine originale (di default i risultati sono tutti i minuscolo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784259it [4:03:54, 53.59it/s] \n"
     ]
    }
   ],
   "source": [
    "symspell_list = []\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    temp_symspell = []\n",
    "    for sentence in row[\"Sentences4\"]:\n",
    "        if any(word in entity_diz_small.keys() for word in sentence.split()):\n",
    "            suggestions = sym_spell.lookup_compound(sentence, max_edit_distance=2, transfer_casing=True)\n",
    "            for suggestion in suggestions:\n",
    "                temp_symspell.append(suggestion.term)\n",
    "        else:\n",
    "            temp_symspell.append(sentence)\n",
    "    symspell_list.append(temp_symspell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Per i futuri lavori sul notebook la lista viene salvata e caricata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('spelling', 'wb') as fp:\n",
    "    pickle.dump(symspell_list, fp)\n",
    "\"\"\"\n",
    "\n",
    "with open('spelling', \"rb\") as input_file:\n",
    "    symspell_list = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Per avere una prima impressione di come abbia lavorato l'algoritmo si crea nuovamente un dizionario in cui si avranno come chiavi le parole non riconosciute dopo la correzione e come valore la loro frequenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 784259/784259 [00:52<00:00, 14909.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unknown_words_spell = {}\n",
    "for i in tqdm(range(len(symspell_list))):\n",
    "    for sentence in symspell_list[i]:\n",
    "        temp = sentence.split(\" \")\n",
    "        for word in temp:\n",
    "            if word.lower() not in word_diz.keys():\n",
    "                if word in unknown_words_spell.keys():\n",
    "                    unknown_words_spell[word] += 1\n",
    "                else:\n",
    "                    unknown_words_spell[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Si stampa un confronto fra i $30$ termini più ricorrenti non riconosciuti in entrambi i dizionari. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ('DVD', 28826)  ('filmmakers', 13303)\n",
      "          ('IMDb', 17913)  ('kinda', 8453)\n",
      "           ('CGI', 17391)  ('Netflix', 6747)\n",
      "    ('filmmakers', 13175)  ('Bollywood', 6719)\n",
      "        ('cliché', 12919)  ('indie', 6656)\n",
      "        ('clichés', 9762)  ('blog', 5178)\n",
      "          ('kinda', 8437)  ('WII', 4683)\n",
      "      ('Bollywood', 6747)  ('UK', 3777)\n",
      "        ('Netflix', 6682)  ('Pixar', 3244)\n",
      "          ('indie', 6611)  ('Godzilla', 2751)\n",
      "        ('clichéd', 6029)  ('DeNiro', 2709)\n",
      "             ('ve', 5012)  ('SRK', 2494)\n",
      "           ('http', 5012)  ('backstory', 2486)\n",
      "             ('ww', 4849)  ('Cannes', 2485)\n",
      "           ('WWII', 4706)  ('Facebook', 2453)\n",
      "             ('UK', 4497)  ('wannabe', 2358)\n",
      "          ('Pixar', 3591)  ('Fargo', 2277)\n",
      "       ('blogspot', 3245)  ('website', 2225)\n",
      "            ('SRK', 3083)  ('screenwriters', 2154)\n",
      "         ('DeNiro', 2947)  ('Brokeback', 2028)\n",
      "             ('WW', 2890)  ('bollywood', 2026)\n",
      "       ('Godzilla', 2763)  ('BBC', 1869)\n",
      "     ('Braveheart', 2727)  ('mindset', 1834)\n",
      "    ('Cloverfield', 2640)  ('Trainspotting', 1783)\n",
      "         ('Cannes', 2490)  ('moviegoers', 1758)\n",
      "            ('NYC', 2490)  ('HBO', 1745)\n",
      "      ('backstory', 2488)  ('Coens', 1720)\n",
      "       ('Facebook', 2462)  ('fairytale', 1719)\n",
      "        ('wannabe', 2374)  ('groundbreaking', 1712)\n",
      "     ('Goodfellas', 2373)  ('favourites', 1689)\n"
     ]
    }
   ],
   "source": [
    "un = Counter(unknown_words).most_common(30)\n",
    "un_spell = Counter(unknown_words_spell).most_common(30)\n",
    "for i in range(len(un)):\n",
    "    print('{:>25}  {:>12}'.format(str(un[i]), str(un_spell[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Le impressioni sulla correzione sono miste: da un lato si ha una frequenza inferiore dei termini, questo indica che diverse parole sono state modificate, anche se non si sa se correttamente, ma ci sono ancora dei termini che non dovrebbero esserci. Inoltre si è notato che la parola <b>DVD</b> è stata corretta con <b>DID</b> in quanto ritenuta la più vicina ($editdistance = 1$). <br></br>\n",
    "\n",
    "Possono esserci diverse interpretazioni del risultato:\n",
    "* Nonostante sia l'algoritmo che dovrebbe offrire il miglior risultato sia in termini di accuratezza sia in termini di prestazioni, non può essere preciso in tutti i casi. Anche per questo metodo l'accuratezza si aggira intorno al $70\\%$, una discreta quantità di parole potrebbe essere sbagliata.\n",
    "* La qualità della correzione si basa sul valore di edit distance massimo impostato. Come è spiegato dall'autore stesso del metodo in [un post](https://medium.com/p/8701fcd87a5f/responses/show) il valore $2$ è quello di default ed è l'unico realmente utilizzato insieme al $3$. I motivi di questa scelta sono:\n",
    "    * La lunghezza media di una parola in inglese è $4.7$, un valore troppo alto restituisce troppi risultati, la maggior parte dei quali non inerenti.\n",
    "    * Il $99\\%$ dei termini è corretto con un edit distance di $2$.\n",
    "    * L'aumento dell'edit distance produce più risultati e quindi aumenta i tempi di lavoro.\n",
    "    \n",
    "Per avere un'idea reale di quanto questo metodo abbia funzionato o meno si dovrà attendere le future analisi. Da qui in poi saranno considerati sia le recensioni pre-processate sia quelle con in più la spelling correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "# [4. POS Tagging](#4)\n",
    "<br></br>\n",
    "## [4.1 NLP Tagger](#4.1)\n",
    "<br></br>\n",
    "<font size=3>\n",
    "Per il POS tagging ci si affida ancora alla libreria `stanfordnlp`. La pipeline viene inizializzata con le due funzioni essenziali per il procedimento, la tokenizzazione ed appunto il tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Hp\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Hp\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\Hp\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(processors = \"tokenize, pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "La struttura dell'algoritmo è la seguente:\n",
    "* Viene passata una frase alla pipeline. Al suo interno sono effettuate le funzioni chiamate in precedenza.\n",
    "* L'output rispecchia quello di ingresso, in questo caso sono state passate in input più frasi in una lista, l'output sono dei tag strutturari nella stessa maniera.\n",
    "* Si accede alle singole parole delle singole frasi e al tag assegnato.\n",
    "\n",
    "Sono restituiti due tipologie di tag come spiegato [nella documentazione](https://stanfordnlp.github.io/stanfordnlp/pos.html) di Stanford: \n",
    "* `UPOS`: Universal POS tags, è una tiplogia di tagging che prende in considerazione $17$ tipologie diverse; solitamente ogni tipologia corrisponde ad una parte del discorso con [qualche eccezione](https://www.sketchengine.eu/universal-pos-tags/).\n",
    "* `XPOS`: Treebank-specific POS è una tipologia più specifica di tagging che identifica $36$ tag distinti. In questo caso sono identificati sottotipi di diverse parti del discordo. [A questo indirizzo](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) è presente la lista completa dei tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Both           upos: CCONJ          xpos: CC            \n",
      "word: Matt           upos: PROPN          xpos: NNP           \n",
      "word: Damon          upos: PROPN          xpos: NNP           \n",
      "word: and            upos: CCONJ          xpos: CC            \n",
      "word: Bales          upos: PROPN          xpos: NNPS          \n",
      "word: performances   upos: NOUN           xpos: NNS           \n",
      "word: were           upos: AUX            xpos: VBD           \n",
      "word: fantastic      upos: ADJ            xpos: JJ            \n",
      "word: and            upos: CCONJ          xpos: CC            \n",
      "word: brilliantly    upos: ADV            xpos: RB            \n",
      "word: directed       upos: VERB           xpos: VBN           \n",
      "word: by             upos: ADP            xpos: IN            \n",
      "word: James          upos: PROPN          xpos: NNP           \n",
      "word: by             upos: ADP            xpos: IN            \n",
      "word: far            upos: ADV            xpos: RB            \n",
      "word: his            upos: PRON           xpos: PRP$          \n",
      "word: best           upos: ADJ            xpos: JJS           \n",
      "word: directed       upos: VERB           xpos: VBN           \n",
      "word: film           upos: NOUN           xpos: NN            \n",
      "word: to             upos: ADP            xpos: IN            \n",
      "word: date           upos: NOUN           xpos: NN            \n",
      "word: Along          upos: ADP            xpos: IN            \n",
      "word: with           upos: ADP            xpos: IN            \n",
      "word: the            upos: DET            xpos: DT            \n",
      "word: excellent      upos: ADJ            xpos: JJ            \n",
      "word: duo            upos: NOUN           xpos: NN            \n",
      "word: Oscar          upos: PROPN          xpos: NNP           \n",
      "word: worthy         upos: ADJ            xpos: JJ            \n",
      "word: performances   upos: NOUN           xpos: NNS           \n",
      "word: the            upos: DET            xpos: DT            \n",
      "word: racing         upos: NOUN           xpos: NN            \n",
      "word: sequences      upos: NOUN           xpos: NNS           \n",
      "word: were           upos: AUX            xpos: VBD           \n",
      "word: upmost         upos: ADJ            xpos: JJ            \n",
      "word: beautifully    upos: ADV            xpos: RB            \n",
      "word: made           upos: VERB           xpos: VBN           \n",
      "word: along          upos: ADP            xpos: IN            \n",
      "word: with           upos: ADP            xpos: IN            \n",
      "word: the            upos: DET            xpos: DT            \n",
      "word: films          upos: NOUN           xpos: NNS           \n",
      "word: cinematography upos: NOUN           xpos: NN            \n",
      "word: Im             upos: AUX            xpos: VBP           \n",
      "word: fully          upos: ADV            xpos: RB            \n",
      "word: confident      upos: ADJ            xpos: JJ            \n",
      "word: and            upos: CCONJ          xpos: CC            \n",
      "word: expecting      upos: VERB           xpos: VBG           \n",
      "word: multiple       upos: ADJ            xpos: JJ            \n",
      "word: Oscar          upos: PROPN          xpos: NNP           \n",
      "word: nominations    upos: NOUN           xpos: NNS           \n",
      "word: on             upos: ADP            xpos: IN            \n",
      "word: all            upos: DET            xpos: DT            \n",
      "word: fronts         upos: NOUN           xpos: NNS           \n",
      "word: for            upos: ADP            xpos: IN            \n",
      "word: this           upos: DET            xpos: DT            \n",
      "word: joyful         upos: ADJ            xpos: JJ            \n",
      "word: movie          upos: NOUN           xpos: NN            \n"
     ]
    }
   ],
   "source": [
    "for sentence in imdb_df[\"Sentences4\"].iloc[10]:\n",
    "    doc = nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            print('{:<20s} {:<20s} {:<20s}'.format('word: ' + word.text, 'upos: ' + word.upos, 'xpos: ' + word.xpos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Per l'aspect-based sentiment analysis non è necessario avere una visione dettagliata di quelli che possono essere i tag, negli articoli pubblicati solitamente si fa riferimento alle parti base del discorso. Viene comunque preferita la classificazione `XPOS` in quanto la notazione dei tag è realizzata in modo tale che i tag specifici di una determinata parte del discorso abbiano lo stesso inizio e delle lettere sono aggiunte in coda per i casi specifici. In questo modo è possibile avere tag specifici ma selezionando solo alla prima parte si ottiene la parte del discorso generale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    for sentence in row[\"Sentences4\"]:\n",
    "        temp_tags = []\n",
    "        try:\n",
    "            doc = nlp(sentence)\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    temp_tags.append(word.xpos)\n",
    "        except AssertionError:\n",
    "            temp_tags.append(\"\")\n",
    "        tags.append(temp_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## [4.2 NLTK + Server](#4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Il principale problema con il metodo precedente è la lentezza computazionale, circa una recensione e mezzo al secondo. Anche per questa procedura Stanford permette di utilizzare un proprio server per velocizzare le operazioni. <br></br>\n",
    "Al contrario del metodo precedente per effettuare questa connessione è necessario utilizzare lo stesso algoritmo di POS tagging di Stanford che è però implementato nella libreria `nltk`:\n",
    "* Basandosi su Java è necessario specificare il percorso in cui l'eseguibile è posizionato.\n",
    "* Si seleziona il modello da importare e il file Java che permetterà di taggare le parole.\n",
    "* Con la funzione `POSClient` presa sempre dal pacchetto `sner` si effettua la connessione al server.\n",
    "\n",
    "Analogamente a quanto fatto per i NER si è dovuto creare una connessione mediante il prompt dei comandi. Il codice utlizzato è stato il seguente: `java -mx300m -cp stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTaggerServer -model english-left3words-distsim.tagger -port 2020`. Gli elementi necessari sono rispettivamente: il file Java, uguale a quello specificato nella variable globale sul notebook, il server a cui collegarsi e il modello da utilizzare, anche in questo caso lo stesso specificato nel notebook.<br></br>\n",
    "\n",
    "Nello .zip scaricato erano presenti due modelli e si utilizza quello meno prestazionale ma circa $10$ volte più veloce come [riportato sul sito](https://stanfordnlp.github.io/CoreNLP/memory-time.html#pos). Il modello scelto ottiene delle performance [leggermente inferiori](https://nlp.stanford.edu/software/pos-tagger-faq.html#h) rispetto a quello migliore ma è quello che garantisce il compromesso migliore tra accuratezza e velocità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath\\\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "model = 'english-left3words-distsim.tagger'\n",
    "jar = 'stanford-postagger.jar'\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf-8') \n",
    "tagger = POSClient(host='localhost', port=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "La struttura dell'algoritmo è simile a quella usata in passato:\n",
    "* Con `iterrows()` si passa una riga alla volta del dataset e si prende una frase alla volta delle recensioni.\n",
    "* Se la frase non è vuota si applica il tagging e dall'output della funzione, una tupla, si accede al secondo elemento che è il tag.\n",
    "* Questo tag viene aggiunto ad una lista temporanea e alla fine della frase si aggiunge tutto alla lista definita fuori dal loop. Nel caso ci sia una frase vuota si aggiunge un elemento nullo.\n",
    "\n",
    "Per questioni di spazio si salva solamente il tag relativo ad ogni termine, per questa ragione è necessario prendere in considerazione anche le frasi rappresentate da stringhe vuote, perché in questa maniera si mantiene la corrispondenza di termine, in una colonna, e tag nella lista appena creata. <br></br>\n",
    "\n",
    "Il risultato sono tante liste quante sono le frasi presenti nel dataset. Come già fatto in precedenza si utilizzerà lo slicing sulle liste per far combaciare più liste alla recensione relativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784259it [3:13:08, 67.68it/s] \n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for index, row in tqdm(imdb_df.iterrows()):\n",
    "    for sentence in row[\"Sentences4\"]:\n",
    "        temp_tags = []\n",
    "        if len(sentence) > 1:\n",
    "            pos_tags = tagger.tag(sentence)\n",
    "            for tag in pos_tags:\n",
    "                temp_tags.append(tag[1])\n",
    "        else:\n",
    "            temp_tags.append(\"\")\n",
    "        tags.append(temp_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Viene replicata la stessa operazione sulle parole corrette. Dato che la lista ottenuta non fa parte del dataset si utilizza un generico loop. L'unica differenza con il codice precedente è che in questo caso si è voluto mantenere la struttura originale:\n",
    "* Ad ogni recensione si crea una nuova lista, elemento inedito rispetto al codice precedente.\n",
    "* Per ogni frase si crea ancora una lista temporanea in cui sono salvati i tag. A differenza di prima, alla fine della frase l'insieme dei tag è convertito in una stringa con la funzione `split()` e gli elementi sono delimitati da uno spazio come se fossero parole. <br></br> Con questa logica con un futuro `split()` sarà semplice avere la corrispondenza esatta fra parole in una lista e tag nell'altra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 784259/784259 [3:08:01<00:00, 69.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tags_spell = []\n",
    "for i in tqdm(range(len(symspell_list))):\n",
    "    review_tags = []\n",
    "    for sentence in symspell_list[i]:\n",
    "        temp_tags = []\n",
    "        if len(sentence) > 1:\n",
    "            pos_tags = tagger.tag(sentence)\n",
    "            for tag in pos_tags:\n",
    "                temp_tags.append(tag[1])\n",
    "        else:\n",
    "            temp_tags.append(\"\")\n",
    "        idx = \" \".join(temp_tags)\n",
    "        review_tags.append(idx)\n",
    "    tags_spell.append(review_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Anche in questo caso per gli ingenti tempi computazali gli output sono salvati con pickle ed è possibile ricaricarli nel loro formato originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('tags_sent', 'wb') as fp:\n",
    "    pickle.dump(tags, fp)\n",
    "\"\"\"\n",
    "    \n",
    "with open('tags_sent', \"rb\") as input_file:\n",
    "    tags_sent = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('tags_spell', 'wb') as fp:\n",
    "    pickle.dump(tags_spell, fp)\n",
    "\"\"\"\n",
    "    \n",
    "with open('tags_spell', \"rb\") as input_file:\n",
    "    tags_spell = pickle.load(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
