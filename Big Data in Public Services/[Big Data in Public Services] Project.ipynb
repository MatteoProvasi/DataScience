{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\prowm\\\\OneDrive\\\\Desktop\\\\Data Science\\\\Public Services\")\n",
    "\n",
    "%run -i \"libraries.py\"\n",
    "%run -i \"functions.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. File reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded comments: 2040273\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"NYT-merge.csv\", sep=\";\", encoding=\"UTF-8\")\n",
    "    data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    print(\"Loaded comments: %s\" %(len(data)))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Merge file not found, loading every .csv\")\n",
    "    \n",
    "    articles_path = \"C:\\\\Users\\\\prowm\\\\OneDrive\\\\Desktop\\\\Data Science\\\\Public Services\\\\Articles\\\\*.csv\"\n",
    "    comments_path = \"C:\\\\Users\\\\prowm\\\\OneDrive\\\\Desktop\\\\Data Science\\\\Public Services\\\\Comments\\\\*.csv\"\n",
    "    articles_list = glob.glob(articles_path)\n",
    "    comments_list = glob.glob(comments_path)\n",
    "\n",
    "    \n",
    "    # Loading data\n",
    "    articles = get_articles(articles_path, articles_list)\n",
    "    comments = get_comments(comments_path, comments_list)\n",
    "\n",
    "    # Merge\n",
    "    data = pd.merge(articles, comments, left_on=\"artID\", right_on=\"comID\", how=\"left\").drop(\"comID\", axis=1)\n",
    "\n",
    "    \n",
    "    data = data.dropna() # around 100k rows do not match any article ID\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(\"index\", axis=1)\n",
    "    data = data[data.Keywords != ''] # remove empty keywords\n",
    "    data = data.drop_duplicates(subset=[\"Comments\"], keep=\"first\")\n",
    "    print(\"Loaded comments: %s\" %(len(data)))\n",
    "\n",
    "    data.to_csv(\"NYT-merge.csv\", sep=\";\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.128*\"Trump, Donald J\" + 0.113*\"United States Politics and Government\" + 0.063*\"Russia\" + 0.043*\"Presidential Election of 2016\" + 0.037*\"Federal Bureau of Investigation\" + 0.029*\"United States International Relations\" + 0.028*\"Comey, James B\" + 0.026*\"Cyberwarfare and Defense\" + 0.023*\"Russian Interference in 2016 US Elections and Ties to Trump Associates\" + 0.019*\"Special Prosecutors (Independent Counsel)\"')\n",
      "(1, '0.099*\"Trump, Donald J\" + 0.082*\"United States Politics and Government\" + 0.042*\"Comey, James B\" + 0.039*\"Federal Bureau of Investigation\" + 0.037*\"Global Warming\" + 0.032*\"United Nations Framework Convention on Climate Change\" + 0.020*\"Crossword Puzzles\" + 0.018*\"United States International Relations\" + 0.017*\"Greenhouse Gas Emissions\" + 0.015*\"Clinton, Hillary Rodham\"')\n",
      "(2, '0.045*\"Trump, Donald J\" + 0.039*\"Politics and Government\" + 0.029*\"United States International Relations\" + 0.019*\"France\" + 0.018*\"United States Politics and Government\" + 0.017*\"Macron, Emmanuel (1977- )\" + 0.017*\"Elections\" + 0.015*\"European Union\" + 0.015*\"Israel\" + 0.013*\"Le Pen, Marine\"')\n",
      "(3, '0.145*\"United States Politics and Government\" + 0.140*\"Trump, Donald J\" + 0.048*\"Republican Party\" + 0.029*\"Health Insurance and Managed Care\" + 0.028*\"Obama, Barack\" + 0.027*\"House of Representatives\" + 0.021*\"Patient Protection and Affordable Care Act (2010)\" + 0.019*\"Ryan, Paul D Jr\" + 0.018*\"United States International Relations\" + 0.013*\"Appointments and Executive Changes\"')\n",
      "(4, '0.070*\"Gun Control\" + 0.070*\"School Shootings and Armed Attacks\" + 0.053*\"Parkland, Fla, Shooting (2018)\" + 0.032*\"National Rifle Assn\" + 0.019*\"Demonstrations, Protests and Riots\" + 0.018*\"Women and Girls\" + 0.015*\"Trump, Donald J\" + 0.015*\"Mental Health and Disorders\" + 0.013*\"Firearms\" + 0.011*\"Youth\"')\n",
      "(5, '0.059*\"Trump, Donald J\" + 0.042*\"United States Politics and Government\" + 0.025*\"Fox News Channel\" + 0.025*\"Deferred Action for Childhood Arrivals\" + 0.023*\"Sexual Harassment\" + 0.019*\"Ethics and Official Misconduct\" + 0.018*\"#MeToo Movement\" + 0.017*\"Illegal Immigration\" + 0.015*\"New York City\" + 0.013*\"Immigration and Emigration\"')\n",
      "(6, '0.122*\"Trump, Donald J\" + 0.118*\"United States Politics and Government\" + 0.043*\"Republican Party\" + 0.039*\"House of Representatives\" + 0.033*\"Senate\" + 0.029*\"Patient Protection and Affordable Care Act (2010)\" + 0.027*\"Health Insurance and Managed Care\" + 0.023*\"Immigration and Emigration\" + 0.021*\"Law and Legislation\" + 0.016*\"Democratic Party\"')\n",
      "(7, '0.029*\"Social Media\" + 0.026*\"Facebook Inc\" + 0.026*\"Supreme Court (US)\" + 0.021*\"Television\" + 0.017*\"Trump, Donald J\" + 0.015*\"Children and Childhood\" + 0.015*\"News and News Media\" + 0.014*\"Law and Legislation\" + 0.014*\"Data-Mining and Database Marketing\" + 0.014*\"Cambridge Analytica\"')\n",
      "(8, '0.080*\"United States Politics and Government\" + 0.061*\"Trump, Donald J\" + 0.027*\"Democratic Party\" + 0.026*\"Elections, House of Representatives\" + 0.023*\"Women and Girls\" + 0.021*\"Labor and Jobs\" + 0.015*\"Midterm Elections (2018)\" + 0.015*\"Republican Party\" + 0.015*\"Global Warming\" + 0.014*\"United States Economy\"')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ldamodel = gensim.models.LdaMulticore.load(\"model-9topic.gensim\")\n",
    "    print_topics=10\n",
    "    topics = ldamodel.print_topics(num_words=print_topics)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: cannot load LDA model, file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_newkey = lda_assign(\"NYT-9topic.csv\", data, ldamodel)\n",
    "#print(np.unique(data_newkey[\"Keywords\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2040273/2040273 [07:55<00:00, 4286.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2040273/2040273 [04:09<00:00, 8174.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 2040273/2040273 [00:42<00:00, 48440.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [13:15<00:00, 271.13s/it]\n"
     ]
    }
   ],
   "source": [
    "data_prep = temp = data_newkey\n",
    "\n",
    "text = pre_processing(temp[\"Comments\"], stopwords=0)\n",
    "data_newkey.drop(\"Comments\", axis=1, inplace=True)\n",
    "data_newkey[\"Comments\"] = text\n",
    "data_newkey.to_csv(\"NYT-NewKeywords.csv\", sep=\";\", encoding=\"UTF-8\") # Saving\n",
    "\n",
    "text_stop = pre_processing(temp[\"Comments\"], stopwords=1)\n",
    "data_prep.drop(\"Comments\", axis=1, inplace=True)\n",
    "data_prep[\"Comments\"] = text_stop\n",
    "\n",
    "# Remove short comments\n",
    "data_prep = remove_short(data_prep, \"Comments\", 9) # Remove\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_prep[\"Comments\"] = lemmatization(data_prep[\"Comments\"])\n",
    "\n",
    "# Remove duplicate comments\n",
    "data_prep = data_prep.drop_duplicates(subset=[\"Comments\"], keep=\"first\")\n",
    "\n",
    "#print(data_prep.head()); print(\"\\n\", Counter(list(data_prep[\"Keywords\"])))\n",
    "\n",
    "\n",
    "data_prep.to_csv(\"NYT-Preprocessing.csv\", sep=\";\", encoding=\"UTF-8\") # Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_newkey = pd.read_csv(\"NYT-NewKeywords.csv\", sep=\";\", encoding=\"UTF-8\")\n",
    "data_newkey = data_newkey.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Classification model comment list and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_newkey[\"Comments\"], \n",
    "                                                    data_newkey[\"Keywords\"],\n",
    "                                                    stratify=data_newkey[\"Keywords\"],\n",
    "                                                    test_size=0.3, random_state=0)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_vect = TfidfVectorizer(min_df=10, ngram_range=(2,2))\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.25, fit_prior=False).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Constructor for sentiment\n",
    "afinn = Afinn()\n",
    "\n",
    "# Full list of comments\n",
    "com_list = list(data_newkey[\"Comments\"])\n",
    "whole_list = [word for row in com_list for word in row.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\prowm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1161192/1161192 [00:05<00:00, 199355.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collecting the tags for each words\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "word_tags = defaultdict(Counter)\n",
    "for word, pos in tqdm(brown.tagged_words(tagset='universal')):\n",
    "    word_tags[word][pos] += 1\n",
    "    \n",
    "\n",
    "brown_tags_words = []\n",
    "for sent in brown.tagged_sents(tagset='universal'): # Simplified version\n",
    "    brown_tags_words.append((\"START\", \"START\"))\n",
    "    brown_tags_words.extend([(tag[:2], word) for (word, tag) in sent])\n",
    "    brown_tags_words.append((\"END\", \"END\"))\n",
    "\n",
    "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
    "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
    "\n",
    "brown_tags = [tag for (tag, word) in brown_tags_words]\n",
    "cfd_tags = nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "    \n",
    "distinct_tags = set(brown_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algorithm to create a new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 200.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a number as the sentiment upper bound 8\n",
      "Sentiment threshold: 8\n",
      "+------------+-------------------+\n",
      "|       Pred | class             |\n",
      "|------------+-------------------|\n",
      "| 0.458383   | Ethics            |\n",
      "| 0.192068   | Laws              |\n",
      "| 0.0937807  | US_Elections_2016 |\n",
      "| 0.0720251  | Guns              |\n",
      "| 0.0594696  | News              |\n",
      "| 0.0462122  | International     |\n",
      "| 0.0429468  | US_Politics       |\n",
      "| 0.0276211  | Environment       |\n",
      "| 0.00749403 | (Social)_Media    |\n",
      "+------------+-------------------+\n",
      "{ ['those', 'who', 'need', 'them', 'and', 'frighten', 'even', 'more', 'people', 'into', 'the', 'shadows', 'i', 'will', 'not', 'answer', 'the', 'citizenship', 'question', 'should'] }\t predicted class: Ethics\n",
      "{ those who need them and frighten even more people into the shadows i will not answer the citizenship question should }\t sentiment score: 0.0 \n",
      "\n",
      "Warning! Adding word with a null sentiment\n",
      "those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked\n",
      "{ those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked }\t sentiment score: 0.0 \n",
      "\n",
      "those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help\n",
      "{ those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help }\t sentiment score: 2.0 \n",
      "\n",
      "those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big\n",
      "{ those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big }\t sentiment score: 3.0 \n",
      "\n",
      "those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big grand opening party\n",
      "{ those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big grand opening party }\t sentiment score: 6.0 \n",
      "\n",
      "those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big grand opening party and\n",
      "{ those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big grand opening party and }\t sentiment score: 6.0 \n",
      "\n",
      "those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big grand opening party and beloved elder\n",
      "New sentence: { those who need them and frighten even more people into the shadows i will not answer the citizenship question should always be asked to help with the big grand opening party and beloved elder }\t score: 9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['those',\n",
       " 'who',\n",
       " 'need',\n",
       " 'them',\n",
       " 'and',\n",
       " 'frighten',\n",
       " 'even',\n",
       " 'more',\n",
       " 'people',\n",
       " 'into',\n",
       " 'the',\n",
       " 'shadows',\n",
       " 'i',\n",
       " 'will',\n",
       " 'not',\n",
       " 'answer',\n",
       " 'the',\n",
       " 'citizenship',\n",
       " 'question',\n",
       " 'should',\n",
       " 'always',\n",
       " 'be',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'help',\n",
       " 'with',\n",
       " 'the',\n",
       " 'big',\n",
       " 'grand',\n",
       " 'opening',\n",
       " 'party',\n",
       " 'and',\n",
       " 'beloved',\n",
       " 'elder']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df           = the dataframe with less preprocessing\n",
    "model        = the best classification model\n",
    "length       = number of words that should be taken to inizialize the sentence\n",
    "comment_list = list of all comments from where to take the words\n",
    "alpha        = smoothing parameters to create an index which combines sentiment and frequence\n",
    "end_seq      = number of element at the end of the sentence that are taken as input for expanding the sentence itself\n",
    "n_match      = minimum number of following sequences found before reducing the end_seq\n",
    "method       = if 0 the function strictly selects part of sentence with not null sentiment\n",
    "               if 1 the function may add pertinent words with null sentiment\n",
    "\"\"\"\n",
    "\n",
    "final_function(df=data_newkey, model=clf, length=20, comment_list=whole_list, \n",
    "                     alpha=0.05, end_seq=8, n_match=3, method=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
